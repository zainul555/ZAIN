{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "os.chdir('C:\\\\Users\\\\zain\\\\OFSTED Data for Assignment')\n",
    "df = pd.read_excel('Combined OFSTED data.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 1\n",
    "df = df[df.Rating != 'Unknown']\n",
    "df['Rating'] = df['Rating'].replace(['Inadequate','Requires Improvement','Satisfactory'],'Not Good')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Good           59.302072\n",
       "Not Good       22.508179\n",
       "Outstanding    18.189749\n",
       "Name: Rating, dtype: float64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Rating.value_counts()/4585*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Good\n",
       "dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Question 2 \n",
    "#baseline accuracy\n",
    "# mode of rating column\n",
    "df.Rating.mode()\n",
    "#We choose mode value for baseline accuracy.In our case the mode is 'Good' and it occurs 59.30% in the dataset.\n",
    "#so we can say that our baseline accuracy is 59.30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "#cols =[ 'PPERSABS10','P8PUP', 'ATT8SCR', 'P8MEA', 'P8CILOW', 'P8CIUPP', 'EBACCAPS',       'PTL2BASICS_94', 'PTL2BASICS_95', 'PTEBACC_E_PTQ_EE', 'PTEBACC_94',       'PTEBACC_95', 'PT5EM_94', 'P8_BANDING', 'OVERALL_DESTPER',       'NOT_SUSTAINEDPER', 'UNKNOWNPER', 'OVERALL_DESTPER_DIS',       'NOT_SUSTAINEDPER_DIS', 'UNKNOWNPER_DIS', 'Rating']\n",
    "#df[cols] = df[cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(to_replace =\"SUPP\",value =\"0\")\n",
    "df = df.replace(to_replace =\"SP\",value =\"0.5\")\n",
    "df = df.replace(to_replace =\"NE\",value =\"0\")\n",
    "df = df.replace(to_replace =\"LOWCOV\",value =\"0\")\n",
    "#df = df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PNORG</th>\n",
       "      <th>PNORB</th>\n",
       "      <th>PSENELSE</th>\n",
       "      <th>PSENELK</th>\n",
       "      <th>PNUMEAL</th>\n",
       "      <th>PNUMENGFL</th>\n",
       "      <th>PNUMUNCFL</th>\n",
       "      <th>PNUMFSMEVER</th>\n",
       "      <th>PERCTOT</th>\n",
       "      <th>PPERSABS10</th>\n",
       "      <th>...</th>\n",
       "      <th>PTEBACC_94</th>\n",
       "      <th>PTEBACC_95</th>\n",
       "      <th>PT5EM_94</th>\n",
       "      <th>P8_BANDING</th>\n",
       "      <th>OVERALL_DESTPER</th>\n",
       "      <th>NOT_SUSTAINEDPER</th>\n",
       "      <th>UNKNOWNPER</th>\n",
       "      <th>OVERALL_DESTPER_DIS</th>\n",
       "      <th>NOT_SUSTAINEDPER_DIS</th>\n",
       "      <th>UNKNOWNPER_DIS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.4</td>\n",
       "      <td>58.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>11.8</td>\n",
       "      <td>73.1</td>\n",
       "      <td>26.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>64.5</td>\n",
       "      <td>6.9</td>\n",
       "      <td>18.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.533</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>96.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.3</td>\n",
       "      <td>10.2</td>\n",
       "      <td>38.1</td>\n",
       "      <td>60.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>49.6</td>\n",
       "      <td>5.3</td>\n",
       "      <td>12.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.749</td>\n",
       "      <td>1</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>45.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>11.1</td>\n",
       "      <td>82.6</td>\n",
       "      <td>17.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>6.7</td>\n",
       "      <td>19.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.526</td>\n",
       "      <td>3</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>47.5</td>\n",
       "      <td>52.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>6.9</td>\n",
       "      <td>55.8</td>\n",
       "      <td>44.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.2</td>\n",
       "      <td>5.4</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.570</td>\n",
       "      <td>3</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33.7</td>\n",
       "      <td>66.3</td>\n",
       "      <td>6.2</td>\n",
       "      <td>13.4</td>\n",
       "      <td>28.2</td>\n",
       "      <td>70.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>47.1</td>\n",
       "      <td>6.3</td>\n",
       "      <td>18.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.625</td>\n",
       "      <td>3</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5652</th>\n",
       "      <td>45.7</td>\n",
       "      <td>54.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>14.5</td>\n",
       "      <td>47.2</td>\n",
       "      <td>45.4</td>\n",
       "      <td>7.5</td>\n",
       "      <td>55.7</td>\n",
       "      <td>7.6</td>\n",
       "      <td>26.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.395</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5660</th>\n",
       "      <td>54.7</td>\n",
       "      <td>45.3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>1.1</td>\n",
       "      <td>98.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>27.6</td>\n",
       "      <td>4.8</td>\n",
       "      <td>10.1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.517</td>\n",
       "      <td>4</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5661</th>\n",
       "      <td>45.8</td>\n",
       "      <td>54.2</td>\n",
       "      <td>1.2</td>\n",
       "      <td>10.4</td>\n",
       "      <td>39.6</td>\n",
       "      <td>59.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>34.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>13.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.516</td>\n",
       "      <td>3</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5743</th>\n",
       "      <td>52.2</td>\n",
       "      <td>47.8</td>\n",
       "      <td>3.7</td>\n",
       "      <td>19.7</td>\n",
       "      <td>2.2</td>\n",
       "      <td>97.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.5</td>\n",
       "      <td>10.1</td>\n",
       "      <td>27.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.356</td>\n",
       "      <td>5</td>\n",
       "      <td>0.84</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5757</th>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>99.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.533</td>\n",
       "      <td>3</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2743 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      PNORG  PNORB  PSENELSE  PSENELK  PNUMEAL  PNUMENGFL  PNUMUNCFL  \\\n",
       "3      41.4   58.6       3.2     11.8     73.1       26.8        0.1   \n",
       "4      96.0    4.0       1.3     10.2     38.1       60.9        0.9   \n",
       "5      45.0   55.0       3.6     11.1     82.6       17.4        0.0   \n",
       "6      47.5   52.5       2.4      6.9     55.8       44.2        0.0   \n",
       "7      33.7   66.3       6.2     13.4     28.2       70.3        1.5   \n",
       "...     ...    ...       ...      ...      ...        ...        ...   \n",
       "5652   45.7   54.3       3.4     14.5     47.2       45.4        7.5   \n",
       "5660   54.7   45.3       1.0     13.7      1.1       98.6        0.4   \n",
       "5661   45.8   54.2       1.2     10.4     39.6       59.0        1.4   \n",
       "5743   52.2   47.8       3.7     19.7      2.2       97.8        0.0   \n",
       "5757   50.0   50.0       1.0      5.6      0.7       99.3        0.0   \n",
       "\n",
       "      PNUMFSMEVER  PERCTOT PPERSABS10  ...  PTEBACC_94  PTEBACC_95  PT5EM_94  \\\n",
       "3            64.5      6.9       18.8  ...       0.191       0.141     0.533   \n",
       "4            49.6      5.3       12.5  ...       0.417       0.326     0.749   \n",
       "5            69.0      6.7       19.2  ...       0.201       0.071     0.526   \n",
       "6            54.2      5.4         14  ...       0.240       0.120     0.570   \n",
       "7            47.1      6.3       18.5  ...       0.368       0.265     0.625   \n",
       "...           ...      ...        ...  ...         ...         ...       ...   \n",
       "5652         55.7      7.6       26.4  ...       0.035       0.009     0.395   \n",
       "5660         27.6      4.8       10.1  ...       0.138       0.090     0.517   \n",
       "5661         34.3      5.2       13.8  ...       0.128       0.080     0.516   \n",
       "5743         54.5     10.1       27.5  ...       0.115       0.067     0.356   \n",
       "5757         18.9      6.0       14.9  ...       0.022       0.015     0.533   \n",
       "\n",
       "      P8_BANDING  OVERALL_DESTPER  NOT_SUSTAINEDPER  UNKNOWNPER  \\\n",
       "3              3              0.9              0.09        0.01   \n",
       "4              1             0.94              0.04        0.02   \n",
       "5              3             0.86              0.08        0.05   \n",
       "6              3             0.91              0.08        0.01   \n",
       "7              3             0.95              0.04        0.01   \n",
       "...          ...              ...               ...         ...   \n",
       "5652           3              0.9              0.07        0.02   \n",
       "5660           4             0.92              0.07        0.01   \n",
       "5661           3             0.91              0.08        0.01   \n",
       "5743           5             0.84              0.15        0.01   \n",
       "5757           3             0.95              0.04        0.01   \n",
       "\n",
       "      OVERALL_DESTPER_DIS  NOT_SUSTAINEDPER_DIS  UNKNOWNPER_DIS  \n",
       "3                    0.89                  0.11            0.01  \n",
       "4                    0.95                  0.04            0.01  \n",
       "5                    0.85                  0.09            0.06  \n",
       "6                    0.87                  0.13               0  \n",
       "7                     0.9                  0.07            0.03  \n",
       "...                   ...                   ...             ...  \n",
       "5652                 0.88                  0.09            0.03  \n",
       "5660                 0.82                  0.16            0.02  \n",
       "5661                  0.8                  0.17            0.03  \n",
       "5743                 0.81                  0.19               0  \n",
       "5757                 0.91                  0.09               0  \n",
       "\n",
       "[2743 rows x 30 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_y = df['Rating']\n",
    "df1_x = df.drop(columns=['Rating','URN','GENDER','RELCHAR','LA','PNUMFSM'])\n",
    "df1_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running different machine learning models\n",
    "K-Nearest Neighbour\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[268  32  19]\n",
      " [ 86  36   2]\n",
      " [ 72   3  31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.63      0.84      0.72       319\n",
      "    Not Good       0.51      0.29      0.37       124\n",
      " Outstanding       0.60      0.29      0.39       106\n",
      "\n",
      "    accuracy                           0.61       549\n",
      "   macro avg       0.58      0.47      0.49       549\n",
      "weighted avg       0.60      0.61      0.58       549\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6102003642987249"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df1_x, df1_y, test_size=0.20)\n",
    "\n",
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[164 118  37]\n",
      " [ 34  88   2]\n",
      " [ 43   0  63]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.68      0.51      0.59       319\n",
      "    Not Good       0.43      0.71      0.53       124\n",
      " Outstanding       0.62      0.59      0.61       106\n",
      "\n",
      "    accuracy                           0.57       549\n",
      "   macro avg       0.58      0.61      0.57       549\n",
      "weighted avg       0.61      0.57      0.58       549\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5737704918032787"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "model1 = GaussianNB()\n",
    "model1.fit(X_train, y_train)\n",
    "y_pred = model1.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[276  28  15]\n",
      " [ 80  44   0]\n",
      " [ 54   0  52]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.67      0.87      0.76       319\n",
      "    Not Good       0.61      0.35      0.45       124\n",
      " Outstanding       0.78      0.49      0.60       106\n",
      "\n",
      "    accuracy                           0.68       549\n",
      "   macro avg       0.69      0.57      0.60       549\n",
      "weighted avg       0.68      0.68      0.66       549\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngupta97\\Desktop\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6775956284153005"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question4 \n",
    "Creating the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels = ['Good', 'Not Good','Outstanding']\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cf_matrix)\n",
    "plt.title('Confusion matrix of the classifier')\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + labels)\n",
    "ax.set_yticklabels([''] + labels)\n",
    "plt.xlabel('Predicted Rating')\n",
    "plt.ylabel('Actual Rating')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question 5\n",
    "We find that there are some features of P8 score having negative values\n",
    "We will treat those features by transforming using min max scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas import DataFrame\n",
    "scaler = MinMaxScaler()\n",
    "normData = pd.DataFrame(scaler.fit_transform(df1_x), index=df1_x.index, columns=df1_x.columns)\n",
    "df1_x = normData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question 6\n",
    "-The first option is using get dummies and creating one hot encoding for each value that is present in the GENDER column\n",
    "- Another option is assigning values to each value present in GENDER column , eg 1- Mixed , 2 - Mixed , 3 - Boys\n",
    "\n",
    "#question 7 \n",
    "\n",
    "-we have merged all the religious values into one category\n",
    "\n",
    "#Question 8,9,10,11 have been covered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_x = df1_x.merge(df[['GENDER','RELCHAR']], left_index=True, right_index=True)\n",
    "\n",
    "\n",
    "df1_x['RELCHAR'] = df1_x['RELCHAR'].replace(['Church of England','Roman Catholic','Christian','Muslim','Islam','Jewish','Anglican','Plymouth Brethren Christian Church','Methodist','Catholic','Church of England/Roman Catholic','Orthodox Jewish','Quaker','Roman Catholic/Church of England','Sikh','Anglican/Christian','Christian/Evangelical','Church of England/Christian','Church of England/Evangelical','Hindu','Anglican/Church of England','Anglican/Evangelical','Protestant/Evangelical','Charadi Jewish','Christian Science','Multi-faith','Christian/Methodist','United Reformed Church','Free Church','Roman Catholic/Anglican','Greek Orthodox','Seventh Day Adventist','Protestant','Sunni Deobandi','Church of England/Methodist','Christian/non-denominational'],'Not Religious')\n",
    "\n",
    "df1_x = pd.get_dummies(df1_x, columns=['GENDER','RELCHAR'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[289  18  17]\n",
      " [ 72  41   0]\n",
      " [ 65   0  47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.68      0.89      0.77       324\n",
      "    Not Good       0.69      0.36      0.48       113\n",
      " Outstanding       0.73      0.42      0.53       112\n",
      "\n",
      "    accuracy                           0.69       549\n",
      "   macro avg       0.70      0.56      0.59       549\n",
      "weighted avg       0.69      0.69      0.66       549\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngupta97\\Desktop\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6867030965391621"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Running the model again with new one hot encoded features\n",
    "#Run 2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df1_x, df1_y, test_size=0.20)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "logreg1 = LogisticRegression()\n",
    "logreg1.fit(X_train, y_train)\n",
    "y_pred = logreg1.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#question 13\n",
    "\n",
    "The main differences between the filter and wrapper methods for feature selection are:\n",
    "\n",
    "-Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "-Filter methods are much faster compared to wrapper methods as they do not involve training the models. On the other hand, wrapper methods are computationally very expensive as well.\n",
    "-Filter methods use statistical methods for evaluation of a subset of features while wrapper methods use cross validation.\n",
    "-Filter methods might fail to find the best subset of features in many occasions but wrapper methods can always provide the best subset of features.\n",
    "-Using the subset of features from the wrapper methods make the model more prone to overfitting as compared to using subset of features from the filter methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question 14\n",
    "\n",
    "Filter methods:\n",
    "-chi-square test\n",
    "-correlation coefficient\n",
    "-variance threshold\n",
    "\n",
    "\n",
    "Variance Thresholds\n",
    "\n",
    "Here, we simply compute the variance of each feature, and we select the subset of features based on a user-specified threshold. E.g., “keep all features that have a variance greater or equal to x” or “keep the the top k features with the largest variance.” We assume that features with a higher variance may contain more useful information, but note that we are not taking the relationship between feature variables or feature and target variables into account, which is one of the drawbacks of filter methods.\n",
    "\n",
    "\n",
    "Correlation Coefficient\n",
    "\n",
    "Features with high correlation are more linearly dependent and hence have almost the same effect on the dependent variable. So, when two features have high correlation, we can drop one of the two features.\n",
    "\n",
    "Chi-squared Score\n",
    "This is another statistical method that’s commonly used for testing relationships between categorical variables.\n",
    "Therefore, it’s suited for categorical variables and binary targets only, and the variables should be non-negative and typically boolean, frequencies, or counts.\n",
    "What it does is simply compare the observed distribution between various features in the dataset and the target variable.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['PSENELSE', 'PSENELK', 'PNUMEAL', 'PNUMFSMEVER', 'PERCTOT',\n",
      "       'PPERSABS10', 'ATT8SCR', 'P8MEA', 'P8CILOW', 'P8CIUPP', 'EBACCAPS',\n",
      "       'PTL2BASICS_94', 'PTL2BASICS_95', 'PTEBACC_E_PTQ_EE', 'PTEBACC_94',\n",
      "       'PTEBACC_95', 'PT5EM_94', 'P8_BANDING', 'NOT_SUSTAINEDPER',\n",
      "       'NOT_SUSTAINEDPER_DIS', 'GENDER_Boys', 'GENDER_Girls', 'GENDER_Mixed',\n",
      "       'RELCHAR_Does not apply', 'RELCHAR_Not Religious'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "# change this to how much features you want to keep from the top ones.\n",
    "select_k = 25\n",
    "\n",
    "# apply the chi2 score on the data and target (target should be binary).  \n",
    "selection = SelectKBest(chi2, k=select_k).fit(X_train, y_train)\n",
    "\n",
    "# display the k selected features.\n",
    "features = X_train.columns[selection.get_support()]\n",
    "print(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df1_x[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[283  24  27]\n",
      " [101  37   0]\n",
      " [ 45   1  31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.66      0.85      0.74       334\n",
      "    Not Good       0.60      0.27      0.37       138\n",
      " Outstanding       0.53      0.40      0.46        77\n",
      "\n",
      "    accuracy                           0.64       549\n",
      "   macro avg       0.60      0.51      0.52       549\n",
      "weighted avg       0.63      0.64      0.61       549\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngupta97\\Desktop\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.639344262295082"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df1_x, df1_y, test_size=0.20)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "logreg1 = LogisticRegression()\n",
    "logreg1.fit(X_train, y_train)\n",
    "y_pred = logreg1.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Question 15 \n",
    "Wrapper method\n",
    "Recursive Feature Elimination (RFE)\n",
    "RFE is a wrapper-type feature selection algorithm. This means that a different machine learning algorithm is given and used in the core of the method, is wrapped by RFE, and used to help select features. This is in contrast to filter-based feature selections that score each feature and select those features with the largest (or smallest) score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngupta97\\Desktop\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:68: FutureWarning: Pass n_features_to_select=25 as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "C:\\Users\\ngupta97\\Desktop\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ngupta97\\Desktop\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ngupta97\\Desktop\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ngupta97\\Desktop\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ngupta97\\Desktop\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ngupta97\\Desktop\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ngupta97\\Desktop\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ngupta97\\Desktop\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ngupta97\\Desktop\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ngupta97\\Desktop\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\ngupta97\\Desktop\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True  True False False  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True False  True  True  True  True\n",
      "  True  True False False  True  True  True False False False False False]\n",
      "[ 6  1  1  2  3  1  1  1  1  1  1  1  1  1  1  1  1  1  1 10  1  1  1  1\n",
      "  1  1 12  4  1  1  1 11  5  7  9  8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ngupta97\\Desktop\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df1_x, df1_y, test_size=0.20)\n",
    "rfe = RFE(logreg1, 25)\n",
    "rfe = rfe.fit(X_train, y_train)\n",
    "# summarize the selection of the attributes\n",
    "print(rfe.support_)\n",
    "print(rfe.ranking_)\n",
    "y_pred = rfe.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[288  20  21]\n",
      " [ 80  45   2]\n",
      " [ 48   0  45]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Good       0.69      0.88      0.77       329\n",
      "    Not Good       0.69      0.35      0.47       127\n",
      " Outstanding       0.66      0.48      0.56        93\n",
      "\n",
      "    accuracy                           0.69       549\n",
      "   macro avg       0.68      0.57      0.60       549\n",
      "weighted avg       0.69      0.69      0.67       549\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6885245901639344"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
